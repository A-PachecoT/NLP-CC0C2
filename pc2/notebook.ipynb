{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2: Clasificación con Transformers preentrenados\n",
    "\n",
    "## Objetivo\n",
    "Clasificación binaria (Positivo vs No Positivo) comparando DistilBERT fine-tuned vs BoW + Logistic Regression.\n",
    "\n",
    "## Historias de usuario\n",
    "- Como científico de datos, quiero comparar transformers vs métodos clásicos para elegir el mejor modelo\n",
    "- Como desarrollador, necesito un modelo eficiente para clasificar sentimientos en producción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "print(\"Setup listo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_csv('data/nlp_prueba_cc0c2.csv')\n",
    "print(f\"Total: {len(df)} muestras\")\n",
    "print(df['Categoría'].value_counts())\n",
    "\n",
    "# Convertir a binario: Positivo vs No Positivo\n",
    "df['label'] = (df['Categoría'] == 'Positivo').astype(int)\n",
    "print(f\"\\nDistribución binaria:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar para entrenamiento (4000) y test (1000)\n",
    "train_df = df.sample(n=4000, random_state=SEED)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "X_train = train_df['Texto'].values\n",
    "y_train = train_df['label'].values\n",
    "X_test = test_df['Texto'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline: BoW + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada 5-fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "bow_scores = []\n",
    "bow_times = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"\\nFold {fold+1}\")\n",
    "    \n",
    "    # Split\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    # Vectorizar\n",
    "    start = time.time()\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    X_tr_vec = vectorizer.fit_transform(X_tr)\n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    \n",
    "    # Entrenar\n",
    "    lr = LogisticRegression(C=1.0, penalty='l2', max_iter=1000, random_state=SEED)\n",
    "    lr.fit(X_tr_vec, y_tr)\n",
    "    \n",
    "    # Evaluar\n",
    "    y_pred = lr.predict(X_val_vec)\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    \n",
    "    tiempo = time.time() - start\n",
    "    bow_scores.append(f1)\n",
    "    bow_times.append(tiempo)\n",
    "    \n",
    "    print(f\"F1: {f1:.4f}, Tiempo: {tiempo:.2f}s\")\n",
    "\n",
    "print(f\"\\nBoW promedio: F1={np.mean(bow_scores):.4f} ± {np.std(bow_scores):.4f}\")\n",
    "print(f\"Tiempo promedio: {np.mean(bow_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo final BoW en todo el train\n",
    "vectorizer_final = CountVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer_final.fit_transform(X_train)\n",
    "X_test_vec = vectorizer_final.transform(X_test)\n",
    "\n",
    "lr_final = LogisticRegression(C=1.0, penalty='l2', max_iter=1000, random_state=SEED)\n",
    "lr_final.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predicciones para ROC/PR\n",
    "y_prob_bow = lr_final.predict_proba(X_test_vec)[:, 1]\n",
    "y_pred_bow = lr_final.predict(X_test_vec)\n",
    "\n",
    "f1_bow = f1_score(y_test, y_pred_bow, average='macro')\n",
    "auc_bow = roc_auc_score(y_test, y_prob_bow)\n",
    "\n",
    "print(f\"BoW Test: F1={f1_bow:.4f}, AUC={auc_bow:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer: DistilBERT multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset para PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar tokenizer y modelo\n",
    "model_name = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(f\"Modelo: {model_name}\")\n",
    "print(f\"Parámetros: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning con una sola partición (simplificado por tiempo)\n",
    "# En producción harías 5-fold completo\n",
    "\n",
    "# Split simple 80/20\n",
    "split_idx = int(0.8 * len(X_train))\n",
    "X_tr, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "y_tr, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "\n",
    "# Datasets\n",
    "train_dataset = TextDataset(X_tr, y_tr, tokenizer)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
    "test_dataset = TextDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,  # Reducido para demo\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "print(\"Entrenando DistilBERT...\")\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "tiempo_bert = time.time() - start\n",
    "print(f\"Tiempo entrenamiento: {tiempo_bert:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en test\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_prob_bert = torch.nn.functional.softmax(torch.from_numpy(predictions.predictions), dim=-1).numpy()[:, 1]\n",
    "y_pred_bert = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "f1_bert = f1_score(y_test, y_pred_bert, average='macro')\n",
    "auc_bert = roc_auc_score(y_test, y_prob_bert)\n",
    "\n",
    "print(f\"BERT Test: F1={f1_bert:.4f}, AUC={auc_bert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curvas ROC y PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular curvas\n",
    "fpr_bow, tpr_bow, _ = roc_curve(y_test, y_prob_bow)\n",
    "fpr_bert, tpr_bert, _ = roc_curve(y_test, y_prob_bert)\n",
    "\n",
    "prec_bow, rec_bow, _ = precision_recall_curve(y_test, y_prob_bow)\n",
    "prec_bert, rec_bert, _ = precision_recall_curve(y_test, y_prob_bert)\n",
    "\n",
    "# Graficar\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC\n",
    "ax1.plot(fpr_bow, tpr_bow, label=f'BoW (AUC={auc_bow:.3f})', color='#3498db')\n",
    "ax1.plot(fpr_bert, tpr_bert, label=f'BERT (AUC={auc_bert:.3f})', color='#e74c3c')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax1.set_xlabel('FPR')\n",
    "ax1.set_ylabel('TPR')\n",
    "ax1.set_title('Curva ROC')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# PR\n",
    "ax2.plot(rec_bow, prec_bow, label=f'BoW', color='#3498db')\n",
    "ax2.plot(rec_bert, prec_bert, label=f'BERT', color='#e74c3c')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Curva Precision-Recall')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/curvas_roc_pr.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparación de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memoria\n",
    "process = psutil.Process()\n",
    "mem_actual = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Tabla comparativa\n",
    "comparacion = pd.DataFrame({\n",
    "    'Modelo': ['BoW + Logistic', 'DistilBERT'],\n",
    "    'F1 Macro': [f1_bow, f1_bert],\n",
    "    'ROC-AUC': [auc_bow, auc_bert],\n",
    "    'Tiempo (s)': [np.mean(bow_times), tiempo_bert],\n",
    "    'Parámetros': ['~5K features', '135M params']\n",
    "})\n",
    "\n",
    "print(\"\\nComparación final:\")\n",
    "print(comparacion.to_string(index=False))\n",
    "\n",
    "# Guardar\n",
    "comparacion.to_csv('out/comparacion_modelos.csv', index=False)\n",
    "print(f\"\\nMemoria pico: {mem_actual:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}